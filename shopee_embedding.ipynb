{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T09:49:52.125056Z",
     "start_time": "2021-03-19T09:49:48.184973Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import sys, os\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T09:49:52.143357Z",
     "start_time": "2021-03-19T09:49:52.139256Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JI52xbMDIIiT",
    "outputId": "04303da1-9da9-4482-a696-bbe130caec38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SEED = 100\n",
    "BATCH_SIZE =128\n",
    "CHUNK_SIZE = 4096\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMjgFzjsHNj4"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T09:49:55.857076Z",
     "start_time": "2021-03-19T09:49:55.851669Z"
    },
    "id": "FE5vX0YWHNj4"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
    "#     image /= 255  # normalize to [0,1] rangeI'm not a \n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)\n",
    "\n",
    "def augmentation(ds):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "#         layers.experimental.preprocessing.RandomTranslation(\n",
    "#             height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n",
    "#         layers.experimental.preprocessing.RandomZoom(0.2, 0.2),\n",
    "    ])\n",
    "    \n",
    "    # Batch all datasets\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # Use data augmentation only on the training set\n",
    "    ds = ds.map(lambda x: data_augmentation(x))\n",
    "\n",
    "    # Prefecting on all datasets\n",
    "    return ds.prefetch(1)\n",
    "\n",
    "def prepare_data(df, augment=False):\n",
    "    # Load images\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(df['image_paths'])\n",
    "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        ds = augmentation(image_ds)\n",
    "    else:\n",
    "        ds = image_ds.batch(BATCH_SIZE).prefetch(1)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T09:50:01.662056Z",
     "start_time": "2021-03-19T09:50:00.819952Z"
    },
    "id": "G6RxdoM8HNj5"
   },
   "outputs": [],
   "source": [
    "load_dir = os.getcwd() + '/data' #'/kaggle/input/ranzcr-clip-catheter-line-classification'\n",
    "\n",
    "# Load and process images\n",
    "df_train = pd.read_csv(load_dir + '/train.csv')\n",
    "df_train['image_paths'] = load_dir + '/train_images/' + df_train['image'] \n",
    "\n",
    "df_test = pd.read_csv(load_dir + '/test.csv')\n",
    "df_test['image_paths'] = load_dir + '/test_images/' + df_test['image'] \n",
    "\n",
    "train_ds = prepare_data(df_train, augment=False)\n",
    "test_ds = prepare_data(df_test, augment=False)\n",
    "\n",
    "# Ground truth\n",
    "tmp = df_train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "df_train['target'] = df_train.label_group.map(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXaljqr3HNj7"
   },
   "source": [
    "# Image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T20:34:59.080196Z",
     "start_time": "2021-03-17T20:33:27.679961Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xM8NdT77HNj7",
    "outputId": "0631c72c-8434-4c41-da09-70de865aa005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/268 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0122s vs `on_predict_batch_end` time: 0.3156s). Check your callbacks.\n",
      "268/268 [==============================] - 86s 321ms/step\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=None)\n",
    "embeddings_image = model.predict(train_ds, verbose=1)\n",
    "\n",
    "del model\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T21:00:10.650675Z",
     "start_time": "2021-03-17T21:00:10.531537Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/image_embeddings.pkl', 'rb') as f:\n",
    "    embeddings_image = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHdxdeptHNj8"
   },
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T21:00:13.217556Z",
     "start_time": "2021-03-17T21:00:13.181775Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLDge51aHNj9",
    "outputId": "39161d0f-5552-4c8a-a703-e3d38bce4099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=50)\n",
    "knn.fit(embeddings_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T15:52:03.660869Z",
     "start_time": "2021-03-17T15:51:19.822647Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6Sg9w4PSDbJ",
    "outputId": "93c686fb-e379-423f-d951-e2368917c20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 to 4096\n",
      "Chunk 4096 to 8192\n",
      "Chunk 8192 to 12288\n",
      "Chunk 12288 to 16384\n",
      "Chunk 16384 to 20480\n",
      "Chunk 20480 to 24576\n",
      "Chunk 24576 to 28672\n",
      "Chunk 28672 to 32768\n",
      "Chunk 32768 to 34250\n"
     ]
    }
   ],
   "source": [
    "image_index = []\n",
    "\n",
    "for i in range(int(np.ceil(len(df_train) / CHUNK_SIZE))):\n",
    "    start_idx = i * CHUNK_SIZE\n",
    "    end_idx = min((i + 1) * CHUNK_SIZE, len(df_train))\n",
    "    print('Chunk', start_idx, 'to', end_idx)\n",
    "\n",
    "    dist, idx = knn.kneighbors(embeddings_image[start_idx:end_idx, :])\n",
    "    counts = (dist < 6.8).sum(axis=1)\n",
    "    chunk_index = [idx[i, :counts[i]].tolist() for i in range(end_idx - start_idx)]\n",
    "    image_index += chunk_index\n",
    "\n",
    "del embeddings_image, dist, idx, counts, chunk_index, knn\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T19:18:57.230025Z",
     "start_time": "2021-03-11T19:18:57.226395Z"
    },
    "id": "ffbllOgsHNj-"
   },
   "source": [
    "# Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:55:32.005604Z",
     "start_time": "2021-03-17T11:55:32.002112Z"
    }
   },
   "outputs": [],
   "source": [
    "# def remove_stopwords(sentence):\n",
    "    \n",
    "#     languages = ['english', 'indonesian']\n",
    "#     cache = set(stopwords.words(languages))\n",
    "#     sentence = ' '.join([word for word in sentence.split() if word not in cache])\n",
    "              \n",
    "#     return sentence\n",
    "\n",
    "# sentences = [remove_stopwords(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T15:52:05.131686Z",
     "start_time": "2021-03-17T15:52:05.128409Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = df_train.title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:41:16.258218Z",
     "start_time": "2021-03-17T14:41:15.218487Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# sequences = tokenizer.texts_to_sequences(sentences)\n",
    "# sentences = tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T16:07:49.342793Z",
     "start_time": "2021-03-17T16:07:49.032787Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(binary=True, max_features=20000)\n",
    "embeddings_text = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:40:54.921574Z",
     "start_time": "2021-03-17T14:40:54.919682Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "# embed2 = hub.KerasLayer('./data/nnlm-id-dim128-with-normalization_2/')\n",
    "# \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "# \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "# \"https://tfhub.dev/google/nnlm-id-dim128-with-normalization/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T15:52:08.590544Z",
     "start_time": "2021-03-17T15:52:08.580659Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_similar_image(embeddings, threshold):\n",
    "    image_index = []\n",
    "\n",
    "    for i in range(int(np.ceil(embeddings.shape[0] / CHUNK_SIZE))):\n",
    "        start_idx = i * CHUNK_SIZE\n",
    "        end_idx = min((i + 1) * CHUNK_SIZE, embeddings.shape[0])\n",
    "\n",
    "        dist, idx = knn.kneighbors(embeddings[start_idx:end_idx, :])\n",
    "        counts = (dist < threshold).sum(axis=1)\n",
    "        chunk_index = [idx[i, :counts[i]].tolist() for i in range(end_idx - start_idx)]\n",
    "        image_index += chunk_index\n",
    "    \n",
    "    return image_index\n",
    "\n",
    "def search_similar_text(embeddings, threshold):\n",
    "\n",
    "    text_index = []\n",
    "\n",
    "    for i in range(int(np.ceil(embeddings.shape[0] / CHUNK_SIZE))):\n",
    "        start_idx = i * CHUNK_SIZE\n",
    "        end_idx = min((i + 1) * CHUNK_SIZE, embeddings.shape[0])\n",
    "\n",
    "        dist, idx = knn_text.kneighbors(embeddings[start_idx:end_idx, :])\n",
    "        counts = (dist < threshold).sum(axis=1)\n",
    "        chunk_index = [idx[i, :counts[i]].tolist() for i in range(end_idx - start_idx)]\n",
    "        text_index += chunk_index\n",
    "    \n",
    "    return text_index\n",
    "\n",
    "\n",
    "def parameter_tuning(df_train, text_index, embeddings, threshold):\n",
    "    print('threshold:', threshold)\n",
    "    image_index = search_similar_image(embeddings, threshold)\n",
    "    df_train['matches'] = [\n",
    "        ' '.join(\n",
    "            set(df_train['posting_id'][text].tolist() +\n",
    "                df_train['posting_id'][image].tolist()))\n",
    "        for text, image in zip(text_index, image_index)\n",
    "    ]\n",
    "    df_train['f1'] = row_wise_f1_score(df_train['target'], df_train['matches'])\n",
    "    f1_score = df_train['f1'].mean()\n",
    "    \n",
    "    print('f1-score:', f1_score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T15:19:22.443209Z",
     "start_time": "2021-03-17T15:08:42.577497Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 6.0\n",
      "f1-score: 0.722843683896605\n",
      "\n",
      "\n",
      "threshold: 6.1\n",
      "f1-score: 0.7232640924332517\n",
      "\n",
      "\n",
      "threshold: 6.199999999999999\n",
      "f1-score: 0.7236380503705018\n",
      "\n",
      "\n",
      "threshold: 6.299999999999999\n",
      "f1-score: 0.7242337449288548\n",
      "\n",
      "\n",
      "threshold: 6.399999999999999\n",
      "f1-score: 0.7243238397305913\n",
      "\n",
      "\n",
      "threshold: 6.499999999999998\n",
      "f1-score: 0.7246730488185004\n",
      "\n",
      "\n",
      "threshold: 6.599999999999998\n",
      "f1-score: 0.7247354028855194\n",
      "\n",
      "\n",
      "threshold: 6.6999999999999975\n",
      "f1-score: 0.7247448445564315\n",
      "\n",
      "\n",
      "threshold: 6.799999999999997\n",
      "f1-score: 0.7248163077903594\n",
      "\n",
      "\n",
      "threshold: 6.899999999999997\n",
      "f1-score: 0.7244262841167836\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(6, 7, 0.1):\n",
    "    parameter_tuning(df_train, text_index, embeddings_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T16:07:57.213405Z",
     "start_time": "2021-03-17T16:07:57.209049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_text = NearestNeighbors(n_neighbors=50)\n",
    "knn_text.fit(embeddings_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T16:08:19.625036Z",
     "start_time": "2021-03-17T16:07:59.313083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 to 4096\n",
      "Chunk 4096 to 8192\n",
      "Chunk 8192 to 12288\n",
      "Chunk 12288 to 16384\n",
      "Chunk 16384 to 20480\n",
      "Chunk 20480 to 24576\n",
      "Chunk 24576 to 28672\n",
      "Chunk 28672 to 32768\n",
      "Chunk 32768 to 34250\n"
     ]
    }
   ],
   "source": [
    "text_index = []\n",
    "\n",
    "for i in range(int(np.ceil(len(df_train) / CHUNK_SIZE))):\n",
    "    start_idx = i * CHUNK_SIZE\n",
    "    end_idx = min((i + 1) * CHUNK_SIZE, len(df_train))\n",
    "    print('Chunk', start_idx, 'to', end_idx)\n",
    "\n",
    "    dist, idx = knn_text.kneighbors(embeddings_text[start_idx:end_idx, :])\n",
    "    counts = (dist < .8).sum(axis=1)\n",
    "    chunk_index = [idx[i, :counts[i]].tolist() for i in range(end_idx - start_idx)]\n",
    "    text_index += chunk_index\n",
    "\n",
    "del embeddings_text, dist, idx, counts, chunk_index, knn_text\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:42:54.131518Z",
     "start_time": "2021-03-17T14:42:54.129523Z"
    },
    "id": "Xx8y4mGdHNj_",
    "outputId": "2b97af23-f44b-45de-b89d-58f9b52c513c"
   },
   "outputs": [],
   "source": [
    "# %memit\n",
    "# text_index = []\n",
    "\n",
    "# for i in range(int(np.ceil(len(embeddings) / CHUNK_SIZE))):\n",
    "#     start_idx = i * CHUNK_SIZE\n",
    "#     end_idx = min((i + 1) * CHUNK_SIZE, len(df_train))\n",
    "#     print('Chunk', start_idx, 'to', end_idx)\n",
    "\n",
    "#     sim_chunk = cosine_similarity(embeddings, embeddings[start_idx:end_idx, :]).T\n",
    "#     r, c = np.where(sim_chunk > 0.95)\n",
    "#     text_index += np.split(c, np.flatnonzero(r[1:] != r[:-1])+1)\n",
    "\n",
    "# del sim_chunk, r, c\n",
    "# _= gc.collect()\n",
    "# %memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvdFHogvHNj_"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T16:08:39.092773Z",
     "start_time": "2021-03-17T16:08:20.631625Z"
    },
    "id": "_02DoVZiHNj_",
    "outputId": "dc7988b6-a894-4fae-9dfa-f37015093a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2097.66 MiB, increment: 0.00 MiB\n",
      "peak memory: 2097.66 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit\n",
    "df_train['matches'] = [\n",
    "    ' '.join(\n",
    "        set(df_train['posting_id'][text].tolist() +\n",
    "            df_train['posting_id'][image].tolist()))\n",
    "    for text, image in zip(text_index, image_index)\n",
    "]\n",
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T16:08:40.310327Z",
     "start_time": "2021-03-17T16:08:40.143822Z"
    },
    "id": "mkgxWwqrHNj_",
    "outputId": "7cbfc72e-b57d-490d-dfd4-c750bbb6f413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7324739189966178"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_wise_f1_score(y_true, y_pred):\n",
    "\n",
    "    y_true = y_true.apply(lambda x: set(x))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = y_pred.apply(lambda x: len(x)).values - tp\n",
    "    fn = y_true.apply(lambda x: len(x)).values - tp\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    return f1\n",
    "\n",
    "df_train['f1'] = row_wise_f1_score(df_train['target'], df_train['matches'])\n",
    "df_train['f1'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2HXapeuHNkA"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T21:29:41.320842Z",
     "start_time": "2021-03-10T21:29:41.311862Z"
    },
    "id": "iett6LwSHNkA",
    "outputId": "20f6ead4-bf1f-4858-f48a-d88b6dc7e116"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_3588702337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_4015706929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id          matches\n",
       "0  test_2255846744  test_2255846744\n",
       "1  test_3588702337  test_3588702337\n",
       "2  test_4015706929  test_4015706929"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submit = pd.read_csv('./data/sample_submission.csv')\n",
    "df_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T21:39:01.331771Z",
     "start_time": "2021-03-10T21:39:01.326078Z"
    },
    "id": "-Gw8C0pSHNkA",
    "outputId": "845cc92c-4efa-4083-9d78-ac603d7bbf62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744 test_4015706929 test_3588702337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_3588702337 test_4015706929 test_2255846744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_4015706929 test_2255846744 test_3588702337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                          matches\n",
       "0  test_2255846744  test_2255846744 test_4015706929 test_3588702337\n",
       "1  test_3588702337  test_3588702337 test_4015706929 test_2255846744\n",
       "2  test_4015706929  test_4015706929 test_2255846744 test_3588702337"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "shopee_embedding_GPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda-env-shopee",
   "language": "python",
   "name": "conda-env-shopee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
