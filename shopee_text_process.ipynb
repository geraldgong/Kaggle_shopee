{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "shopee_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDY25bjjOpSE"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UIwGlQFOmTt",
        "outputId": "f40df6c8-1332-48df-fe2e-be62dd3007fb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 31 06:54:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrx_9WH8PwfG",
        "outputId": "7f329d03-23bd-42ef-e8df-8cde933b4ccb"
      },
      "source": [
        "# Install RAPIDS\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!bash rapidsai-csp-utils/colab/rapids-colab.sh stable\n",
        "\n",
        "import sys, os\n",
        "\n",
        "dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n",
        "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n",
        "sys.path\n",
        "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 205 (delta 16), reused 3 (delta 0), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (205/205), 62.72 KiB | 2.73 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "PLEASE READ\n",
            "********************************************************************************************************\n",
            "Changes:\n",
            "1. IMPORTANT SCRIPT CHANGES: Colab has updated to Python 3.7, and now runs our STABLE and NIGHTLY versions (0.18 and 0.19)!  PLEASE update your older install script code as follows:\n",
            "\t!bash rapidsai-csp-utils/colab/rapids-colab.sh 0.18\n",
            "\n",
            "\timport sys, os\n",
            "\n",
            "\tdist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n",
            "\tsys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n",
            "\tsys.path\n",
            "\texec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())\n",
            "\n",
            "2. IMPORTANT NOTICE: CuGraph's Louvain requires a Volta+ GPU (T4, V100).  If you get a P4 or P100 and intend to use Louvain, please FACTORY RESET your instance and try to get a compatible GPU\n",
            "3. Default stable version is now 0.18.  Nightly is now 0.19.\n",
            "3. You can declare your RAPIDSAI version as a CLI option and skip the user prompts (ex: '0.18' or '0.19', between 0.17 to 0.19, without the quotes): \n",
            "        \"!bash rapidsai-csp-utils/colab/rapids-colab.sh <version/label>\"\n",
            "        Examples: '!bash rapidsai-csp-utils/colab/rapids-colab.sh 0.18', or '!bash rapidsai-csp-utils/colab/rapids-colab.sh stable', or '!bash rapidsai-csp-utils/colab/rapids-colab.sh s'\n",
            "                  '!bash rapidsai-csp-utils/colab/rapids-colab.sh 0.19, or '!bash rapidsai-csp-utils/colab/rapids-colab.sh nightly', or '!bash rapidsai-csp-utils/colab/rapids-colab.sh n'\n",
            "Enjoy using RAPIDS!  If you have any issues with or suggestions for RAPIDSAI on Colab, please create a bug request on https://github.com/rapidsai/rapidsai-csp-utils/issues/new.\n",
            "For a near instant entry into a RAPIDS Library experience, or if we haven't fixed a fatal issue yet, please use https://app.blazingsql.com/.  Thanks!\n",
            "Starting to prep Colab for install RAPIDS Version 0.18 stable\n",
            "Checking for GPU type:\n",
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a Tesla P100-PCIE-16GB!\n",
            "***********************************************************************\n",
            "\n",
            "Removing conflicting packages, will replace with RAPIDS compatible versions\n",
            "Uninstalling dask-2.12.0:\n",
            "  Successfully uninstalled dask-2.12.0\n",
            "Uninstalling distributed-1.25.3:\n",
            "  Successfully uninstalled distributed-1.25.3\n",
            "Uninstalling xgboost-0.90:\n",
            "  Successfully uninstalled xgboost-0.90\n",
            "Uninstalling pyarrow-3.0.0:\n",
            "  Successfully uninstalled pyarrow-3.0.0\n",
            "Uninstalling numba-0.51.2:\n",
            "  Successfully uninstalled numba-0.51.2\n",
            "Uninstalling llvmlite-0.34.0:\n",
            "  Successfully uninstalled llvmlite-0.34.0\n",
            "Uninstalling PySocks-1.7.1:\n",
            "  Successfully uninstalled PySocks-1.7.1\n",
            "Uninstalling requests-2.23.0:\n",
            "  Successfully uninstalled requests-2.23.0\n",
            "Uninstalling six-1.15.0:\n",
            "  Successfully uninstalled six-1.15.0\n",
            "Uninstalling urllib3-1.24.3:\n",
            "  Successfully uninstalled urllib3-1.24.3\n",
            "Uninstalling cffi-1.14.5:\n",
            "  Successfully uninstalled cffi-1.14.5\n",
            "Installing conda\n",
            "--2021-03-31 06:55:13--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94235922 (90M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>]  89.87M   186MB/s    in 0.5s    \n",
            "\n",
            "2021-03-31 06:55:13 (186 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [94235922/94235922]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - brotlipy==0.7.0=py38h27cfd23_1003\n",
            "    - ca-certificates==2020.10.14=0\n",
            "    - certifi==2020.6.20=pyhd3eb1b0_3\n",
            "    - cffi==1.14.3=py38h261ae71_2\n",
            "    - chardet==3.0.4=py38h06a4308_1003\n",
            "    - conda-package-handling==1.7.2=py38h03888b9_0\n",
            "    - conda==4.9.2=py38h06a4308_0\n",
            "    - cryptography==3.2.1=py38h3c74f83_1\n",
            "    - idna==2.10=py_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20191231=h14c3975_1\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_1\n",
            "    - openssl==1.1.1h=h7b6447c_0\n",
            "    - pip==20.2.4=py38h06a4308_0\n",
            "    - pycosat==0.6.3=py38h7b6447c_1\n",
            "    - pycparser==2.20=py_2\n",
            "    - pyopenssl==19.1.0=pyhd3eb1b0_1\n",
            "    - pysocks==1.7.1=py38h06a4308_0\n",
            "    - python==3.8.5=h7579374_1\n",
            "    - readline==8.0=h7b6447c_0\n",
            "    - requests==2.24.0=py_0\n",
            "    - ruamel_yaml==0.15.87=py38h7b6447c_1\n",
            "    - setuptools==50.3.1=py38h06a4308_1\n",
            "    - six==1.15.0=py38h06a4308_0\n",
            "    - sqlite==3.33.0=h62c20be_0\n",
            "    - tk==8.6.10=hbc83047_0\n",
            "    - tqdm==4.51.0=pyhd3eb1b0_0\n",
            "    - urllib3==1.25.11=py_0\n",
            "    - wheel==0.35.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py38h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.10.14-0\n",
            "  certifi            pkgs/main/noarch::certifi-2020.6.20-pyhd3eb1b0_3\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.3-py38h261ae71_2\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py38h06a4308_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.9.2-py38h06a4308_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.7.2-py38h03888b9_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-3.2.1-py38h3c74f83_1\n",
            "  idna               pkgs/main/noarch::idna-2.10-py_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20191231-h14c3975_1\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1h-h7b6447c_0\n",
            "  pip                pkgs/main/linux-64::pip-20.2.4-py38h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py38h7b6447c_1\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.20-py_2\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-19.1.0-pyhd3eb1b0_1\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38h06a4308_0\n",
            "  python             pkgs/main/linux-64::python-3.8.5-h7579374_1\n",
            "  readline           pkgs/main/linux-64::readline-8.0-h7b6447c_0\n",
            "  requests           pkgs/main/noarch::requests-2.24.0-py_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py38h7b6447c_1\n",
            "  setuptools         pkgs/main/linux-64::setuptools-50.3.1-py38h06a4308_1\n",
            "  six                pkgs/main/linux-64::six-1.15.0-py38h06a4308_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.33.0-h62c20be_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.10-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.51.0-pyhd3eb1b0_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.25.11-py_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.35.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Installing RAPIDS 0.18 packages from the stable release channel\n",
            "Please standby, this will take a few minutes...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKn6PvkPCVM5",
        "outputId": "8d2a8a17-dc5a-4107-ef27-6134d82a0c60"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PXMAAo4Ob9W"
      },
      "source": [
        "## Download Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TRfrQpqQh4L"
      },
      "source": [
        "# Download data and move to Google Drive\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c shopee-product-matching\n",
        "!mv shopee-product-matching.zip ./drive/MyDrive/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLG6xYNrhsm"
      },
      "source": [
        "# Copy from Google Drive and unzip\n",
        "!cp ./drive/MyDrive/data/shopee-product-matching.zip ./\n",
        "!unzip -q shopee-product-matching.zip\n",
        "!rm shopee-product-matching.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3O88-z7O2f6"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OdpNGzkP_zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5d704b-31b6-46ba-a5a7-13d77667c471"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "from cuml.feature_extraction.text import TfidfVectorizer\n",
        "from cuml.cluster import DBSCAN\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cudf, cuml, cupy\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcFptIELQbTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b908dae3-bedf-479c-cd79-014c2c81b08b"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "SEED = 100\n",
        "BATCH_SIZE = 32\n",
        "CHUNK_SIZE = 4096\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "LIMIT = 2.0\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(physical_devices))\n",
        "\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        physical_devices[0], [\n",
        "            tf.config.experimental.VirtualDeviceConfiguration(\n",
        "                memory_limit=1024 * LIMIT)\n",
        "        ])\n",
        "    print('TensorFlow usage is restricted to max %iGB GPU RAM' % LIMIT)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs: 1\n",
            "TensorFlow usage is restricted to max 2GB GPU RAM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpxXtUgPD17"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z9VmKGTfxpx"
      },
      "source": [
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "#     image /= 255  # normalize to [0,1] rangeI'm not a \n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image)\n",
        "\n",
        "def augmentation(ds):\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.3),\n",
        "      layers.experimental.preprocessing.RandomTranslation(\n",
        "          height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.2, 0.2),\n",
        "    ])\n",
        "    \n",
        "    # Batch all datasets\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set\n",
        "    ds = ds.map(lambda x: data_augmentation(x))\n",
        "\n",
        "    # Prefecting on all datasets\n",
        "    return ds.prefetch(1)\n",
        "\n",
        "def prepare_data(df, augment=False):\n",
        "    # Load images\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(df['image_paths'])\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        ds = augmentation(image_ds)\n",
        "    else:\n",
        "        ds = image_ds.batch(BATCH_SIZE).prefetch(1)\n",
        "    \n",
        "    return ds"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly3wRLopf2Tp"
      },
      "source": [
        "load_dir = os.getcwd()\n",
        "# Load and process images\n",
        "df_train = pd.read_csv(load_dir + '/train.csv')\n",
        "df_train['image_paths'] = load_dir + '/train_images/' + df_train['image'] \n",
        "\n",
        "# Ground truth\n",
        "tmp = df_train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
        "df_train['target'] = df_train.label_group.map(tmp)\n",
        "\n",
        "# Duplicate train set for runtime testing \n",
        "# df_train = pd.concat([df_train, df_train], ignore_index=True)\n",
        "train_ds = prepare_data(df_train, augment=False)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-2RN2wg8EJ"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJoEvr6tWeg"
      },
      "source": [
        "class SimilarSearch:\n",
        "    \"\"\"\n",
        "    Finding the similar embeddings with NearestNeighbors or Cosine similarity.\n",
        "    Dynamically learning the threshold for each item with DBSCAN.\n",
        "    RAPIDS.AI accelerated.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=50):\n",
        "        self.CHUNK_SIZE = 4096 \n",
        "        self.k = k\n",
        "        self.MIN_SAMPLES = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def dbscan_cluster(dbscan, idx, dist, strict=False):\n",
        "        \"\"\"\n",
        "        Figure out number of similar items by excluding the last cluster.\n",
        "\n",
        "        Parameters:\n",
        "            dbscan: DBSCAN model\n",
        "            idx: index of nearest items\n",
        "            dist: distance between items \n",
        "        Returns:\n",
        "            index: index of similar items\n",
        "        \"\"\"\n",
        "        dbscan.fit(dist.reshape(-1, 1))\n",
        "        labels = dbscan.labels_\n",
        "    \n",
        "        if strict:\n",
        "            counts = len(labels[labels==0])\n",
        "        else:\n",
        "            counts = len(\n",
        "                labels[:labels.argmax()]) if labels.max() != 0 else len(labels)\n",
        "        \n",
        "        similar_item_index = idx[:counts].tolist()\n",
        "        \n",
        "        return similar_item_index\n",
        "\n",
        "    def nearest_neighbors(self, embeddings, min_dist, verbose=True):\n",
        "        \"\"\"\n",
        "        Get the k nearest items.\n",
        "\n",
        "        Parameters:\n",
        "            embeddings: embeddings of images\n",
        "            min_dist: the maximum distance between two samples (DBSCAN)\n",
        "            verbose: wether to print out chunk verbose\n",
        "        Returns:\n",
        "            item_index: indexes of similar items\n",
        "        \"\"\"\n",
        "        knn = NearestNeighbors(n_neighbors=self.k)\n",
        "        knn.fit(embeddings)\n",
        "        dbscan = DBSCAN(eps=min_dist, min_samples=self.MIN_SAMPLES)\n",
        "\n",
        "        num_chunk = round(embeddings.shape[0] / self.CHUNK_SIZE)\n",
        "        item_index = []\n",
        "        for i in range(num_chunk + 1):\n",
        "            start_idx = i * self.CHUNK_SIZE\n",
        "            end_idx = min((i + 1) * self.CHUNK_SIZE, embeddings.shape[0])\n",
        "            if verbose:\n",
        "                print('Chunk', start_idx, 'to', end_idx)\n",
        "\n",
        "            dist, idx = knn.kneighbors(embeddings[start_idx:end_idx, :])\n",
        "            for j in range(end_idx - start_idx):\n",
        "                index = self.dbscan_cluster(dbscan, idx[j], dist[j])\n",
        "                item_index.append(index)\n",
        "\n",
        "        return item_index\n",
        "\n",
        "    def cosine_similarity(self, embeddings, threshold, verbose=True):\n",
        "        \"\"\"\n",
        "        Get similar items with cosine similarity, applicable for unit vectors.\n",
        "        \"\"\"\n",
        "        num_chunk = round(embeddings.shape[0] / self.CHUNK_SIZE)\n",
        "        item_index = []\n",
        "        for i in range(num_chunk + 1):\n",
        "            start_idx = i * self.CHUNK_SIZE                  \n",
        "            end_idx = min((i + 1) * self.CHUNK_SIZE, embeddings.shape[0])\n",
        "            if verbose:\n",
        "                print('Chunk', start_idx, 'to', end_idx) \n",
        "            \n",
        "            cos_sim = cupy.matmul(embeddings, embeddings[start_idx:end_idx].T).T\n",
        "            for j in range(end_idx - start_idx):\n",
        "                idx = cupy.where(cos_sim[j] > threshold)[0]\n",
        "                item_index.append(cupy.asnumpy(idx))\n",
        "\n",
        "        return item_index\n",
        "\n",
        "sim = SimilarSearch()"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9wXuNMusDp"
      },
      "source": [
        "## Image Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbbSumnkgtrB",
        "outputId": "b28f88df-0ea1-4dd6-9759-c4a0c351a387"
      },
      "source": [
        "# Image embedding\n",
        "effnet = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=None)\n",
        "embeddings_image = effnet.predict(train_ds, verbose=1)\n",
        "\n",
        "del effnet\n",
        "_ = gc.collect()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1071/1071 [==============================] - 50s 46ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb8EM-EtMjip",
        "outputId": "e1a924cc-a794-4e01-fce4-14227049b53a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Image Similarity Search:\")\n",
        "image_index = sim.nearest_neighbors(embeddings_image, 1.2)\n",
        "\n",
        "del embeddings_image\n",
        "_ = gc.collect()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Similarity Search:\n",
            "Chunk 0 to 4096\n",
            "Chunk 4096 to 8192\n",
            "Chunk 8192 to 12288\n",
            "Chunk 12288 to 16384\n",
            "Chunk 16384 to 20480\n",
            "Chunk 20480 to 24576\n",
            "Chunk 24576 to 28672\n",
            "Chunk 28672 to 32768\n",
            "Chunk 32768 to 34250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNcCeKEnuw2u"
      },
      "source": [
        "## Text Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSGb3MjPiQOb"
      },
      "source": [
        "# Text embedding\n",
        "cudf_train = cudf.DataFrame(df_train)\n",
        "sentences = cudf_train.title\n",
        "vectorizer = TfidfVectorizer(binary=True, max_features=25000)\n",
        "embeddings_text = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "del sentences, vectorizer\n",
        "_ = gc.collect()"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_XdgusxWYQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623a9339-4be5-4a08-adac-a5a54503ebd6"
      },
      "source": [
        "print(f\"Text Similarity Search:\")\n",
        "text_index = sim.cosine_similarity(embeddings_text, 0.72)\n",
        "\n",
        "del embeddings_text\n",
        "_ = gc.collect()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Similarity Search:\n",
            "Chunk 0 to 4096\n",
            "Chunk 4096 to 8192\n",
            "Chunk 8192 to 12288\n",
            "Chunk 12288 to 16384\n",
            "Chunk 16384 to 20480\n",
            "Chunk 20480 to 24576\n",
            "Chunk 24576 to 28672\n",
            "Chunk 28672 to 32768\n",
            "Chunk 32768 to 34250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ps0OTSuvk0M"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcFH0yMYv12s",
        "outputId": "306f8eb7-44e2-4b48-939a-f3beb86ca61e"
      },
      "source": [
        "def row_wise_f1_score(y_true, y_pred):\n",
        "\n",
        "    y_true = y_true.apply(lambda x: set(x))\n",
        "    y_pred = y_pred.apply(lambda x: set(x))\n",
        "\n",
        "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    fp = y_pred.apply(lambda x: len(x)).values - tp\n",
        "    fn = y_true.apply(lambda x: len(x)).values - tp\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
        "    return f1\n",
        "\n",
        "\n",
        "def evaluation(df_data, image_index, text_index):\n",
        "    df_data['matches'] = [\n",
        "        set(df_data['posting_id'][text].tolist() +\n",
        "            df_data['posting_id'][image].tolist())\n",
        "        for text, image in zip(text_index, image_index)\n",
        "    ]\n",
        "\n",
        "    df_data['f1'] = row_wise_f1_score(df_data['target'], df_data['matches'])\n",
        "    return df_data['f1'].mean()\n",
        "\n",
        "\n",
        "print('f1-score:', evaluation(df_train, image_index, text_index))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1-score: 0.7463547261512918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eP7vh3Y61hE"
      },
      "source": [
        "# Parameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQADMjcKIkt3"
      },
      "source": [
        "def parameter_tuning(df_data, embeddings_text, image_index, \n",
        "                     threshold):\n",
        "  \n",
        "    text_index = sim.cosine_similarity(embeddings_text, threshold, verbose=False)\n",
        "    f1_score = evaluation(df_data, image_index, text_index)\n",
        "    print('threshold_text: {}, f1-score: {}'.format(\n",
        "        round(threshold, 2), round(f1_score, 4)))\n",
        "\n",
        "    return f1_score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-xBJQ2JF7-",
        "outputId": "65856145-1e2c-47cf-c9fe-d100639b69e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results = {}\n",
        "for i in np.arange(.06, 0.15, .01):\n",
        "    f1 = parameter_tuning(df_train, embeddings_text, image_index, i)\n",
        "    results[round(i,2)] = f1\n",
        "\n",
        "print('The best thresholds is {}, yield a f1-score of {}'.format(max(results, key=results.get), max(results.values())))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "threshold_text: 0.06, f1-score: 0.7114\n",
            "threshold_text: 0.07, f1-score: 0.7254\n",
            "threshold_text: 0.08, f1-score: 0.7352\n",
            "threshold_text: 0.09, f1-score: 0.7412\n",
            "threshold_text: 0.1, f1-score: 0.7449\n",
            "threshold_text: 0.11, f1-score: 0.747\n",
            "threshold_text: 0.12, f1-score: 0.7471\n",
            "threshold_text: 0.13, f1-score: 0.7465\n",
            "threshold_text: 0.14, f1-score: 0.7437\n",
            "The best thresholds is 0.12, yield a f1-score of 0.7470603407640282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61wncFM673S"
      },
      "source": [
        "# Results Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeKvNfA3662O"
      },
      "source": [
        "def results_inspection(df_train, df_example, col_name):\n",
        "\n",
        "    df_target = df_train[df_train.posting_id.isin(df_example[col_name])] \n",
        "    print('Number of items:', len(df_target))\n",
        "    print(df_target.title.values)\n",
        "    n_img = len(df_target)\n",
        "    n_cols = 5 if n_img >= 5 else n_img\n",
        "    n_rows = n_img // n_cols if n_cols <5 else (n_img // n_cols) + 1\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image_path, in enumerate(df_target['image_paths']):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = preprocess_image(image) / 255\n",
        "        plt.subplot(n_rows, n_cols, i+1)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "df_train['image_matches'] = [df_train['posting_id'][image].tolist() for image in image_index]\n",
        "df_train['text_matches'] = [df_train['posting_id'][text].tolist() for text in text_index]\n",
        "\n",
        "df_perfect = df_train[df_train.f1==1]\n",
        "print('Number of perfect matches:', len(df_perfect))\n",
        "df_bad = df_train[df_train.f1 < 0.7]\n",
        "print('Number of bad matches:', len(df_bad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_fSjTZK7SbO"
      },
      "source": [
        "## Good Match Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sxkd9hb7Cul"
      },
      "source": [
        "df_example = df_perfect.iloc[random.randint(0, len(df_perfect))] # index: 33119\n",
        "print('Ground Truth:')\n",
        "results_inspection(df_train, df_example, 'target')\n",
        "print('Image Embeddings:')\n",
        "results_inspection(df_train, df_example, 'image_matches')\n",
        "print('Text Embeddings:')\n",
        "results_inspection(df_train, df_example, 'text_matches')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6P6pmpy7Jac"
      },
      "source": [
        "## Bad Match Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqFIFtG27Ou7"
      },
      "source": [
        "df_example = df_bad.iloc[random.randint(0, len(df_bad))]\n",
        "print('Ground Truth:')\n",
        "results_inspection(df_train, df_example, 'target')\n",
        "print('Image Embeddings:')\n",
        "results_inspection(df_train, df_example, 'image_matches')\n",
        "print('Text Embeddings:')\n",
        "results_inspection(df_train, df_example, 'text_matches')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxulpogwMLVO"
      },
      "source": [
        "# Test Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYhudL6-MKSU"
      },
      "source": [
        "def remove_bad_chars(sentences):\n",
        "    # Remove characters e.g. \\\\xc3\\\\x89\n",
        "    return [re.sub(r'\\\\x[a-z0-9]{2}', ' ', sentence) for sentence in sentences]\n",
        "\n",
        "def remove_parenthesized_content(sentences):\n",
        "    # Remove contect in () or []\n",
        "    return [re.sub(r'\\[[^)]*\\]|\\([^)]*\\)', ' ', sentence) for sentence in sentences]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXtw1rfBMO5p"
      },
      "source": [
        "sentences = remove_bad_chars(title)\n",
        "sentences = remove_parenthesized_content(sentences)\n",
        "df_train['title'] = sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nONM2LTNMaFG"
      },
      "source": [
        "cache = set(stopwords.words(['english', 'indonesian', 'german']))\n",
        "title = df_train.title.tolist()\n",
        "sentences = [' '.join(word for word in sentence.split() if word not in cache) for sentence in title]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTNcOG-aMaoD"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(title)\n",
        "sequences = tokenizer.texts_to_sequences(title)\n",
        "sentences = tokenizer.sequences_to_texts(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}