{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArgFace_EfficientNetB4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNaqqX7VVs9OZkdu/fXx561",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geraldgong/Kaggle_shopee/blob/gong/ArgFace_EfficientNetB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrSlCWvqJBMX",
        "outputId": "071609fb-d295-4aeb-b92b-2d1f579f23ce"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 10 09:41:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieliQAivJCs2"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy2yu-jQJKkS",
        "outputId": "36405ff3-c5c2-47a2-b6cd-913fc5aa166b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OnLS3xJMyU"
      },
      "source": [
        "## Get Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlbuuzm0Dlf9",
        "outputId": "c5ac4174-c4dc-4b06-b072-cd9692fa7553",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8f8e67d0-a8e2-4c3a-94bf-eb324c410bf4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8f8e67d0-a8e2-4c3a-94bf-eb324c410bf4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1o2bI5ENOh"
      },
      "source": [
        "!kaggle datasets download -d ragnar123/shopee-tf-records-512-stratified\n",
        "!unzip -q ./shopee-tf-records-512-stratified.zip"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ_vjA1cJS0r"
      },
      "source": [
        "# Copy from Google Drive and unzip\n",
        "!cp ./drive/MyDrive/data/shopee-product-matching.zip ./\n",
        "!unzip -q shopee-product-matching.zip\n",
        "!rm shopee-product-matching.zip\n",
        "\n",
        "!cp ./drive/MyDrive/data/universal-sentence-encoder-cmlm_multilingual-preprocess_2.tar.gz ./\n",
        "!cp ./drive/MyDrive/data/2.tar.gz ./\n",
        "!mkdir preprocessor\n",
        "!mkdir labse_model\n",
        "!tar -xf universal-sentence-encoder-cmlm_multilingual-preprocess_2.tar.gz -C ./preprocessor\n",
        "!tar -xf 2.tar.gz -C ./labse_model\n",
        "!rm universal-sentence-encoder-cmlm_multilingual-preprocess_2.tar.gz 2.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwSL7b5aJb-Z"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXkRd3d-Jd47"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJyKfu_HJkeb",
        "outputId": "8c41f7e4-2eba-47b8-969c-d897f146fcb7"
      },
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(physical_devices))\n",
        "\n",
        "strategy = tf.distribute.get_strategy()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbmkU1EfJoJ_"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pK3_WTfkTOA"
      },
      "source": [
        "## Prepare .tfrec files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdLcdELZJnZe",
        "outputId": "34082fc5-f5f6-4d84-d0e0-486d0fa5b0ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def _bytes_feature(value):\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def serialize_example(feature0, feature1, feature2, feature3, feature4, feature5):\n",
        "    feature = {\n",
        "        'posting_id': _bytes_feature(feature0.encode('utf-8')),\n",
        "        'image': _bytes_feature(feature1),\n",
        "        'image_name': _bytes_feature(feature2.encode('utf-8')),\n",
        "        'image_phash': _bytes_feature(feature3.encode('utf-8')),\n",
        "        'title': _bytes_feature(feature4.encode('utf-8')),\n",
        "        'label_group': _int64_feature(feature5)\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "PATH = '/content/train_images/'\n",
        "IMGS = list(df.image)\n",
        "SIZE = 2048\n",
        "CT = len(IMGS)//SIZE + int(len(IMGS)%SIZE!=0)\n",
        "\n",
        "for j in tqdm(range(CT)):\n",
        "    CT2 = min(SIZE, len(IMGS) - j*SIZE) \n",
        "    with tf.io.TFRecordWriter('./train_tfrec/train%.2i-%i.tfrec'%(j, CT2)) as writer: \n",
        "        for k in range(CT2): \n",
        "            img = cv2.imread(PATH+IMGS[j*SIZE + k])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n",
        "            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tobytes()\n",
        "\n",
        "            name = IMGS[SIZE*j+k]\n",
        "            row = df.loc[df.image==name]\n",
        "            example = serialize_example(\n",
        "                row.posting_id.values[0],\n",
        "                img,\n",
        "                name,\n",
        "                row.image_phash.values[0],\n",
        "                row.title.values[0],\n",
        "                row.label_group.values[0])\n",
        "    \n",
        "            writer.write(example)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:35<09:33, 35.82s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [01:11<08:56, 35.75s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [01:48<08:25, 36.12s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [02:24<07:50, 36.18s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [03:00<07:13, 36.10s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [03:36<06:36, 36.09s/it]\u001b[A\n",
            " 41%|████      | 7/17 [04:12<05:59, 35.99s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [04:47<05:21, 35.68s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [05:22<04:44, 35.54s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [05:58<04:08, 35.51s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [06:33<03:33, 35.53s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [07:09<02:58, 35.71s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [07:45<02:22, 35.67s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [08:21<01:47, 35.72s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [08:56<01:11, 35.61s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [09:32<00:35, 35.60s/it]\u001b[A\n",
            "100%|██████████| 17/17 [09:57<00:00, 35.16s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb2zY2MbkbMe"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ADxLPyLkRhQ"
      },
      "source": [
        "# Data access\n",
        "GCS_PATH = '/content'\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "# Configuration\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [512, 512]\n",
        "# Seed\n",
        "SEED = 42\n",
        "# Learning rate\n",
        "LR = 0.001\n",
        "# Verbosity\n",
        "VERBOSE = 1\n",
        "# Number of classe\n",
        "N_CLASSES = 11014\n",
        "# Number of folds\n",
        "FOLDS = 5\n",
        "\n",
        "# Training filenames directory\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay__7qZCE-bd",
        "outputId": "a10f2fcb-2c24-4b4e-8372-dd04ceec0450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Function to get our f1 score\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_true = y_true.apply(lambda x: set(x.split()))\n",
        "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
        "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
        "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
        "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
        "    return f1\n",
        "\n",
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def arcface_format(posting_id, image, label_group, matches):\n",
        "    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n",
        "\n",
        "# Data augmentation function\n",
        "def data_augment(posting_id, image, label_group, matches):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_hue(image, 0.01)\n",
        "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
        "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
        "    image = tf.image.random_brightness(image, 0.10)\n",
        "    return posting_id, image, label_group, matches\n",
        "\n",
        "# Function to decode our images\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# This function parse our images and also get the target variable\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"matches\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    posting_id = example['posting_id']\n",
        "    image = decode_image(example['image'])\n",
        "#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n",
        "    label_group = tf.cast(example['label_group'], tf.int32)\n",
        "    matches = example['matches']\n",
        "    return posting_id, image, label_group, matches\n",
        "\n",
        "# This function loads TF Records and parse them into tensors\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_training_dataset(filenames, ordered = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# This function is to get our validation tensors\n",
        "def get_validation_dataset(filenames, ordered = True):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "# Function to count how many photos we have in\n",
        "def count_data_items(filenames):\n",
        "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 34250 training images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpJTrFnNYzsD"
      },
      "source": [
        "# Image Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drQb0qazUg4B"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieX11vhlYy4I"
      },
      "source": [
        "# Function for a custom learning rate scheduler with warmup and decay\n",
        "def get_lr_callback():\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * BATCH_SIZE\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 5\n",
        "    lr_decay   = 0.8\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if epoch < 5:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "    return lr_callback\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "\n",
        "# Function to create our EfficientNetB3 model\n",
        "def get_model():\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.7, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        x = EfficientNetB3(weights = 'imagenet', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        x = margin([x, label])\n",
        "        \n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "\n",
        "def build_model():\n",
        "    margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.7, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "    x = EfficientNetB3(weights = None, include_top = False)(inp)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = margin([x, label])\n",
        "        \n",
        "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "    model.load_weights('/content/EfficientNetB3_512_42.h5')\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = opt,\n",
        "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        ) \n",
        "\n",
        "    return model "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA4nitzXUkT5"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRUSimKOZmnv",
        "outputId": "03a0b3f8-5033-41e5-9d5e-8868ef464f1c"
      },
      "source": [
        "# Seed everything\n",
        "seed_everything(SEED)\n",
        "\n",
        "print('\\n')\n",
        "print('-'*50)\n",
        "\n",
        "train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
        "train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
        "\n",
        "STEPS_PER_EPOCH = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
        "K.clear_session()\n",
        "# model = get_model()\n",
        "# Model checkpoint\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB3_{IMAGE_SIZE[0]}_{SEED}.h5', \n",
        "                                                monitor = 'loss', \n",
        "                                                verbose = VERBOSE, \n",
        "                                                save_best_only = True,\n",
        "                                                save_weights_only = True, \n",
        "                                                mode = 'min')\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                    epochs = 10,\n",
        "                    callbacks = [checkpoint, get_lr_callback()], \n",
        "                    verbose = VERBOSE)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Epoch 1/10\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 4e-05.\n",
            "4281/4281 [==============================] - 2364s 547ms/step - loss: 21.4322 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from inf to 21.29479, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 2/10\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 4e-05.\n",
            "4281/4281 [==============================] - 2345s 548ms/step - loss: 20.2776 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: loss improved from 21.29479 to 20.09633, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 3/10\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 4e-05.\n",
            "4281/4281 [==============================] - 2344s 547ms/step - loss: 19.6507 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: loss improved from 20.09633 to 19.54295, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 4/10\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 4e-05.\n",
            "4281/4281 [==============================] - 2355s 550ms/step - loss: 19.3059 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: loss improved from 19.54295 to 19.23431, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 5/10\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 4e-05.\n",
            "4281/4281 [==============================] - 2333s 545ms/step - loss: 19.1031 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: loss improved from 19.23431 to 19.05082, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00012001855468749998.\n",
            "4281/4281 [==============================] - 2346s 548ms/step - loss: 19.4643 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00006: loss did not improve from 19.05082\n",
            "Epoch 7/10\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 9.621484375e-05.\n",
            "4281/4281 [==============================] - 2346s 548ms/step - loss: 19.1668 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00007: loss did not improve from 19.05082\n",
            "Epoch 8/10\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 7.7171875e-05.\n",
            "4281/4281 [==============================] - 2335s 545ms/step - loss: 18.8811 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00008: loss improved from 19.05082 to 18.81824, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 6.193750000000001e-05.\n",
            "4281/4281 [==============================] - 2349s 549ms/step - loss: 18.6957 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00009: loss improved from 18.81824 to 18.65153, saving model to EfficientNetB3_512_42.h5\n",
            "Epoch 10/10\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 4.975e-05.\n",
            "4281/4281 [==============================] - 2345s 548ms/step - loss: 18.5567 - sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00010: loss improved from 18.65153 to 18.52415, saving model to EfficientNetB3_512_42.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSvk_tkNVmAb"
      },
      "source": [
        "!cp ./EfficientNetB3_512_42.h5 ./drive/MyDrive/data/"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}